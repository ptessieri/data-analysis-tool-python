{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74ef45a7eec4a8abeb25eea8ad35efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Data Type:', options=('Blobs', 'Moons', 'Swiss Roll', 'S-Curve', 'â€¦"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import gaussian_kde\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# --------------------------\n",
    "# DATA GENERATION FUNCTIONS\n",
    "# --------------------------\n",
    "\n",
    "def generate_blobs(n_samples=300):\n",
    "    \"\"\"Generate clustered blob data.\"\"\"\n",
    "    return datasets.make_blobs(n_samples=n_samples, centers=3, cluster_std=1.0, n_features=3, random_state=42)[0]\n",
    "\n",
    "def generate_moons(n_samples=300):\n",
    "    \"\"\"Generate moon-shaped data.\"\"\"\n",
    "    data, _ = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
    "    return np.c_[data, np.zeros(n_samples)]  # Add a dummy zero third dimension\n",
    "\n",
    "def generate_swiss_roll(n_samples=300):\n",
    "    \"\"\"Generate swiss roll data.\"\"\"\n",
    "    data, _ = datasets.make_swiss_roll(n_samples=n_samples, noise=0.05)\n",
    "    return data\n",
    "\n",
    "def generate_s_curve(n_samples=300):\n",
    "    \"\"\"Generate S-curve data.\"\"\"\n",
    "    data, _ = datasets.make_s_curve(n_samples=n_samples, noise=0.05)\n",
    "    return data\n",
    "\n",
    "def generate_random(n_samples=300):\n",
    "    \"\"\"Generate random data.\"\"\"\n",
    "    return np.random.rand(n_samples, 3)\n",
    "\n",
    "def generate_wine():\n",
    "    \"\"\"Generate wine data and project it to 3 dimensions using PCA.\"\"\"\n",
    "    wine_data = datasets.load_wine().data\n",
    "    pca = PCA(n_components=3)\n",
    "    transformed_wine_data = pca.fit_transform(wine_data)\n",
    "    return transformed_wine_data\n",
    "\n",
    "def generate_high_dim_small_sample():\n",
    "    \"\"\"Generate high dimensional small sample data.\"\"\"\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    return np.random.randn(50, 100)\n",
    "\n",
    "def generate_time_series(n_samples=300):\n",
    "    \"\"\"Generate synthetic time series data.\"\"\"\n",
    "    t = np.linspace(0, 4 * np.pi, n_samples)\n",
    "    return np.sin(t) + 0.5 * np.random.randn(n_samples)\n",
    "\n",
    "\n",
    "# Available datasets\n",
    "DATA_OPTIONS = {\n",
    "    \"Blobs\": generate_blobs,\n",
    "    \"Moons\": generate_moons,\n",
    "    \"Swiss Roll\": generate_swiss_roll,\n",
    "    \"S-Curve\": generate_s_curve,\n",
    "    \"Random\": generate_random,\n",
    "    \"Wine\": generate_wine,\n",
    "    \"High Dimensional Small Sample\": generate_high_dim_small_sample,\n",
    "    \"Time Series\": generate_time_series\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# UTILITY FUNCTIONS\n",
    "# --------------------------\n",
    "\n",
    "def gaussian_similarity_matrix(data, sigma):\n",
    "    \"\"\"Compute the Gaussian similarity matrix.\"\"\"\n",
    "    pairwise_dists = np.sum(data**2, 1).reshape(-1, 1) + np.sum(data**2, 1) - 2 * data @ data.T\n",
    "    return np.exp(-pairwise_dists / (2 * sigma**2))\n",
    "\n",
    "def compute_laplacian(similarity_matrix):\n",
    "    \"\"\"Compute the Laplacian matrix.\"\"\"\n",
    "    D = np.diag(np.sum(similarity_matrix, axis=1))\n",
    "    return D - similarity_matrix\n",
    "\n",
    "def generate_vector_field(laplacian, data):\n",
    "    \"\"\"Generate the vector field using Fiedler vector (2nd smallest eigenvector of Laplacian).\"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eigh(laplacian)\n",
    "    fiedler_vector = eigvecs[:, 1]\n",
    "    X, Y, Z = data[:, 0], data[:, 1], data[:, 2]\n",
    "    U, V, W = fiedler_vector, fiedler_vector, fiedler_vector\n",
    "    return X, Y, Z, U, V, W\n",
    "\n",
    "def decompose_time_series(time_series, period=12):\n",
    "    \"\"\"\n",
    "    Decomposes a time series into its trend, seasonal, and residual components.\n",
    "    Args:\n",
    "    - time_series (array-like): The time series data.\n",
    "    - period (int): The frequency for the seasonal decomposition.\n",
    "    \n",
    "    Returns:\n",
    "    - result (DecomposeResult): The decomposition result containing trend, seasonal, and residual components.\n",
    "    \"\"\"\n",
    "    result = seasonal_decompose(time_series, model='additive', period=period)\n",
    "    return result\n",
    "\n",
    "# --------------------------\n",
    "# VISUALIZATION FUNCTION\n",
    "# --------------------------\n",
    "\n",
    "def visualize(data_type, sigma, n_clusters, bin_size):\n",
    "    \"\"\"Main visualization function.\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
    "        \n",
    "        # Generate data\n",
    "        data = DATA_OPTIONS[data_type]()\n",
    "\n",
    "        if data_type == \"Time Series\":\n",
    "            # Decompose the time series\n",
    "            decomposition = decompose_time_series(data)\n",
    "\n",
    "            # Plot the original time series, trend, seasonal, and residual components\n",
    "            fig, axes = plt.subplots(4, 1, figsize=(10, 8))\n",
    "\n",
    "            axes[0].plot(data, label=\"Original\")\n",
    "            axes[0].legend(loc=\"best\")\n",
    "            axes[0].set_title(\"Original Time Series\")\n",
    "\n",
    "            axes[1].plot(decomposition.trend, label=\"Trend\")\n",
    "            axes[1].legend(loc=\"best\")\n",
    "            axes[1].set_title(\"Trend Component\")\n",
    "\n",
    "            axes[2].plot(decomposition.seasonal, label=\"Seasonal\")\n",
    "            axes[2].legend(loc=\"best\")\n",
    "            axes[2].set_title(\"Seasonal Component\")\n",
    "\n",
    "            axes[3].plot(decomposition.resid, label=\"Residual\")\n",
    "            axes[3].legend(loc=\"best\")\n",
    "            axes[3].set_title(\"Residual Component\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            return  # Exit the function to avoid other visualizations for time series data\n",
    "\n",
    "        # Initialize a figure for plotting\n",
    "        fig = plt.figure(figsize=(16, 20))\n",
    "\n",
    "        # --------------------------\n",
    "        # 1. PCA Visualization\n",
    "        # --------------------------\n",
    "        \n",
    "        ax1 = fig.add_subplot(3, 2, 1, projection='3d')\n",
    "        n_samples, n_features = data.shape\n",
    "\n",
    "        # Use Gram matrix if number of samples is less than number of features\n",
    "        if n_samples < n_features:\n",
    "            G = np.dot(data, data.T)\n",
    "            eigvals, eigvecs = np.linalg.eigh(G)\n",
    "            pca_data = np.dot(eigvecs.T, data)\n",
    "        else:\n",
    "            pca_model = PCA(n_components=3)\n",
    "            pca_data = pca_model.fit_transform(data)\n",
    "\n",
    "        sc1 = ax1.scatter(pca_data[:, 0], pca_data[:, 1], pca_data[:, 2], c=pca_data[:, 2], cmap='rainbow', marker='o', s=100, alpha=0.6)\n",
    "\n",
    "        # Plot eigenvectors if we used the standard PCA approach\n",
    "        if n_samples >= n_features:\n",
    "            for length, vector in zip(pca_model.explained_variance_, pca_model.components_):\n",
    "                v = vector * 3 * np.sqrt(length)\n",
    "                ax1.quiver(0, 0, 0, v[0], v[1], v[2], arrow_length_ratio=0.1, color='red')\n",
    "            ax1.text2D(0.05, 0.95, f\"Explained Variance Ratios: {pca_model.explained_variance_ratio_}\", transform=ax1.transAxes)\n",
    "        \n",
    "        ax1.set_title('PCA Visualization with Eigenvectors')\n",
    "        fig.colorbar(sc1, ax=ax1, label='PCA Component 3 Value')\n",
    "\n",
    "        # --------------------------\n",
    "        # 2. Spectral Clustering Visualization\n",
    "        # --------------------------\n",
    "        \n",
    "        ax2 = fig.add_subplot(3, 2, 2, projection='3d')\n",
    "\n",
    "        # Compute similarity matrix and Laplacian\n",
    "        similarity_matrix = gaussian_similarity_matrix(data, sigma)\n",
    "        laplacian_matrix = compute_laplacian(similarity_matrix)\n",
    "\n",
    "        # Perform spectral clustering\n",
    "        clustering_labels = SpectralClustering(n_clusters=n_clusters, affinity='precomputed').fit_predict(similarity_matrix)\n",
    "\n",
    "        sc2 = ax2.scatter(data[:, 0], data[:, 1], data[:, 2], c=clustering_labels, cmap='rainbow', marker='o', s=100, alpha=0.6)\n",
    "\n",
    "        # Compute silhouette score\n",
    "        if n_samples < n_features:\n",
    "            silhouette_avg = silhouette_score(pca_data[:, :3], clustering_labels)  # Use PCA-reduced data for silhouette\n",
    "        else:\n",
    "            silhouette_avg = silhouette_score(data, clustering_labels)\n",
    "        \n",
    "        ax2.text2D(0.05, 0.95, f\"Silhouette Score: {silhouette_avg:.2f}\", transform=ax2.transAxes)\n",
    "        ax2.set_title('Spectral Clustering Visualization')\n",
    "        fig.colorbar(sc2, ax=ax2, label='Cluster ID')\n",
    "\n",
    "        # --------------------------\n",
    "        # 3. Original Data with Vector Field\n",
    "        # --------------------------\n",
    "        \n",
    "        ax3 = fig.add_subplot(3, 2, 3, projection='3d')\n",
    "        \n",
    "        # Generate vector field using Fiedler vector\n",
    "        X, Y, Z, U, V, W = generate_vector_field(laplacian_matrix, data)\n",
    "        \n",
    "        sc3 = ax3.scatter(data[:, 0], data[:, 1], data[:, 2], c=data[:, 2], cmap='rainbow', marker='o', s=100, alpha=0.6)\n",
    "        ax3.quiver(X, Y, Z, U, V, W, length=0.1, normalize=True, color='k', linewidth=0.5, alpha=0.5)\n",
    "        ax3.set_title('Original Data with Vector Field and Eigenvectors')\n",
    "        fig.colorbar(sc3, ax=ax3, label='Z-axis Value')\n",
    "\n",
    "        # Display basic statistics\n",
    "        metrics = [\n",
    "            f\"X-axis: Min: {data[:,0].min():.2f}, Max: {data[:,0].max():.2f}, Mean: {data[:,0].mean():.2f}\",\n",
    "            f\"Y-axis: Min: {data[:,1].min():.2f}, Max: {data[:,1].max():.2f}, Mean: {data[:,1].mean():.2f}\",\n",
    "            f\"Z-axis: Min: {data[:,2].min():.2f}, Max: {data[:,2].max():.2f}, Mean: {data[:,2].mean():.2f}\"\n",
    "        ]\n",
    "        for i, metric in enumerate(metrics):\n",
    "            ax3.text2D(0.05, 0.95 - i * 0.05, metric, transform=ax3.transAxes)\n",
    "\n",
    "        # --------------------------\n",
    "        # 4. KDE Visualization\n",
    "        # --------------------------\n",
    "        \n",
    "        ax4 = fig.add_subplot(3, 2, 4, projection='3d')\n",
    "        \n",
    "        # Compute and visualize KDE if applicable\n",
    "        variance_threshold = 1e-4\n",
    "        is_variance_high_enough = np.all(np.var(data, axis=0) > variance_threshold)\n",
    "        is_more_samples_than_features = data.shape[0] >= data.shape[1]\n",
    "\n",
    "        if is_variance_high_enough and is_more_samples_than_features:\n",
    "            kde_estimator = gaussian_kde(data.T)\n",
    "            density_values = kde_estimator(data.T)\n",
    "            sc4 = ax4.scatter(data[:, 0], data[:, 1], data[:, 2], c=density_values, cmap='Reds', s=100)\n",
    "            fig.colorbar(sc4, ax=ax4, label='Density')\n",
    "        else:\n",
    "            ax4.text2D(0.5, 0.5, \"KDE not applicable for this dataset\", transform=ax4.transAxes, ha='center')\n",
    "        \n",
    "        ax4.set_title('KDE Visualization')\n",
    "\n",
    "        # --------------------------\n",
    "        # 5. Histogram Visualization\n",
    "        # --------------------------\n",
    "        \n",
    "        ax5 = fig.add_subplot(3, 2, 5, projection='3d')\n",
    "        \n",
    "        # Compute and visualize histogram based on data dimensionality\n",
    "        if data.shape[1] == 3:\n",
    "            hist_data, edges_data = np.histogramdd(data, bins=(bin_size, bin_size, bin_size))\n",
    "        elif data.shape[1] == 2:\n",
    "            hist_data, edges_data = np.histogram2d(data[:, 0], data[:, 1], bins=(bin_size, bin_size))\n",
    "            hist_data = np.expand_dims(hist_data, axis=2)  # Add a dummy third dimension for the histogram\n",
    "            edges_data = list(edges_data) + [np.array([0, 1])]\n",
    "        else:\n",
    "            hist_data, edges_data = np.histogram(data, bins=bin_size)\n",
    "            hist_data = np.expand_dims(hist_data, axis=(1,2))  # Add dummy dimensions for 3D visualization\n",
    "            edges_data = [edges_data, np.array([0, 1]), np.array([0, 1])]\n",
    "        \n",
    "        x_edges, y_edges, z_edges = edges_data\n",
    "        x_positions, y_positions, z_positions = np.meshgrid(x_edges[:-1], y_edges[:-1], z_edges[:-1], indexing=\"ij\")\n",
    "        x_positions = x_positions.flatten()\n",
    "        y_positions = y_positions.flatten()\n",
    "        z_positions = z_positions.flatten()\n",
    "        dx = dy = dz = bin_size * np.ones_like(z_positions)\n",
    "        ax5.bar3d(x_positions, y_positions, z_positions, dx, dy, hist_data.flatten(), shade=True, color='cyan', zsort='average')\n",
    "        ax5.set_title('Histogram Visualization')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# INTERACTIVE WIDGET\n",
    "# --------------------------\n",
    "\n",
    "widgets.interactive(\n",
    "    visualize,\n",
    "    data_type=widgets.Dropdown(options=DATA_OPTIONS.keys(), value='Blobs', description='Data Type:'),\n",
    "    sigma=widgets.FloatSlider(value=1.0, min=0.1, max=5.0, step=0.1, description='Sigma:'),\n",
    "    n_clusters=widgets.IntSlider(value=3, min=2, max=10, description='Clusters:'),\n",
    "    bin_size=widgets.IntSlider(value=10, min=1, max=50, description='Bin Size:')\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
